NeMA-LC â€” Neural Memory Allocation with Lifecycle Control

Author: Sudipta Nath

ğŸ“Œ Overview

NeMA-LC introduces a learned memory lifecycle framework for memory-augmented Transformers, enabling models to write, retain, update, and forget memory slots under a fixed capacity budget.

Unlike prior approaches that treat memory as a passive buffer or rely on heuristic replacement strategies (e.g., FIFO, LRU), NeMA-LC learns explicit lifecycle decisions through a neural controller, transforming memory into an actively managed computational resource.

This repository contains the full implementation, training pipeline, and analysis code for Paper 2.

ğŸ§  Core Research Question

How can a neural model dynamically manage external memoryâ€”deciding when to write, retain, update, or forgetâ€”under a fixed budget, while preserving long-range task performance?

âœ¨ Key Contributions

NeMA-LC makes the following contributions:

Memory Lifecycle Formulation
Formalises memory management as a learned lifecycle:
write â†’ retain â†’ update â†’ forget

Neural Memory Controller
A controller network predicts per-slot lifecycle actions conditioned on:

memory content

age

usage

current context

Explicit Budget Constraint
Slot operations and write actions compete under a shared budget, preventing uncontrolled memory growth.

Lifecycle-Aware Training Objective
Introduces explicit losses for:

write budget control

forgetting cost

memory stability (churn)

Empirical Analysis of Memory Dynamics
Goes beyond accuracy by analysing:

memory utilisation

average retention age

write / update / forget rates

stability over long horizons

ğŸ§© Relationship to Paper 1
Paper	Focus	Scope
Paper 1 (NeMA-Lite)	When to write?	Selective memory writing
Paper 2 (NeMA-LC)	How to manage memory over time?	Full lifecycle control

Paper 2 generalises and subsumes Paper 1 by addressing the complete memory lifecycle.

ğŸ—ï¸ Architecture Overview

NeMA-LC consists of four core components:

Transformer Encoder
Produces contextual representations from long sequences.

Fixed-Budget Memory Slots
Each slot stores:

content vector

age

usage signal

alive state

Neural Memory Controller
Predicts per-slot probabilities for:

retain

update

forget
and a global write score.

Budgeted Allocator
Enforces a hard limit on the total number of memory operations per step.

ğŸ“‚ Repository Structure
nema-paper2/
â”œâ”€â”€ README.md
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ models/
â”‚   â”‚   â”œâ”€â”€ memory_slots.py        # Fixed-size memory + metadata
â”‚   â”‚   â”œâ”€â”€ memory_controller.py   # Lifecycle controller + allocator
â”‚   â”‚   â””â”€â”€ transformer_lc.py      # Transformer + lifecycle memory
â”‚   â”œâ”€â”€ train.py                   # Training loop (toy + LRA)
â”‚   â””â”€â”€ eval.py                    # Evaluation utilities
â”œâ”€â”€ scripts/
â”‚   â”œâ”€â”€ plot_memory_dynamics.py    # Memory dynamics plots
â”‚   â””â”€â”€ plot_lra_compare.py        # Baseline vs NeMA-LC comparison
â”œâ”€â”€ logs/                          # (ignored) training logs
â”œâ”€â”€ plots/                         # (ignored) generated figures
â””â”€â”€ requirements.txt

ğŸ§ª Supported Tasks

Toy Long-Context Classification (sanity checks)

LRA Retrieval (long-range benchmark)

The framework is task-agnostic and designed to extend to:

document QA

continual learning

multimodal memory (Paper 3)

ğŸš€ Running Experiments
1ï¸âƒ£ Setup Environment
python -m venv .venv
source .venv/bin/activate
pip install -r requirements.txt

2ï¸âƒ£ Train LRA Baseline (no memory)
python -m src.train --task lra --batch_size 4

3ï¸âƒ£ Train NeMA-LC on LRA
python -m src.train --task lra --use_memory --batch_size 4

4ï¸âƒ£ Plot Memory Dynamics
python scripts/plot_memory_dynamics.py


This generates:

memory utilisation curves

average retention age

lifecycle operation rates

loss component trajectories

5ï¸âƒ£ Compare Against Baseline
python scripts/plot_lra_compare.py


Produces paper-ready comparison plots.

ğŸ“Š Logged Metrics

NeMA-LC logs the following per training step:

task loss

write loss

forget loss

stability loss

memory utilisation

average memory age

write / update / forget rates

accuracy

These metrics support interpretability and ablation analysis, not just performance reporting.

ğŸ§  Design Choices (Important Notes)

Memory Reset per Batch
Memory is reset at batch boundaries to model episodic memory.
This avoids cross-batch gradient entanglement and enables controlled analysis.

Explicit Budget Enforcement
Write actions are not free â€” they compete with slot operations.

Interpretability First
Memory dynamics are treated as first-class experimental results.

ğŸ† Target Publication Venues

This work is intended for Q1 journals, including:

IEEE Transactions on Neural Networks and Learning Systems (TNNLS)

Machine Learning Journal (Springer)

Neural Computation (MIT Press)

Transactions on Machine Learning Research (TMLR)

ğŸ“Œ Status

âœ” Core architecture implemented
âœ” Lifecycle losses integrated
âœ” Long-context benchmark validated
âœ” Memory dynamics analysed

ğŸŸ¡ Additional benchmarks (e.g., Document QA) optional
ğŸŸ¡ Ablation study recommended before submission

ğŸ“– Citation (placeholder)
@article{nema_lc,
  title={Neural Memory Allocation with Lifecycle Control},
  author={},
  journal={},
  year={}
}

ğŸ§­ Roadmap

Paper 2 (this work): Learned memory lifecycle control

Paper 3: Continual and multimodal memory systems

Thesis: Unified neural memory systems for long-horizon reasoning

âœ… Bottom line

This repository contains a complete, reproducible, and journal-ready implementation of NeMA-LC, addressing a fundamental open problem in memory-augmented neural networks.
